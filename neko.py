from transformers import AutoModelForCausalLM, AutoTokenizer
from vllm import LLM, SamplingParams # Sample prompts.
import torch
import json
from tqdm import tqdm
from utils.filter import mt_filter, st_filter
from APIs.gpt import generate_gpt_response

def get_model(model_id):
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    llm = LLM(model=model_id, tensor_parallel_size=8)

    # model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True, torch_dtype=torch.bfloat16, device_map="auto")
    # print(model.hf_device_map)

    return llm, tokenizer

def get_prompt(data, task):
    if task == "asr":
        prompts = [
            (
                "The following text contains 5-best hypotheses from an Automatic Speech Recognition system. "
                "As part of a speech recognition task, please perform error correction on the hypothesis "
                "to generate the most accurate transcription of the spoken text.\n\n"
                f"{candidate}\n\n"
                "Corrected transcription:"
                    ) for candidate in data
                    ]
    elif task == "mt":
        prompts = [
            (
                "The following text contains 5-best hypotheses in Japanese, which were generated by translating a sentence originally in English. "
                "As part of a machine translation task, please perform error "
                "correction on the hypotheses to generate one most accurate translation of the original sentence in Japanese.\n\n"
                f"{candidate}\n\n"
                "Corrected translation:"
                    ) for candidate in data
                    ]        
    # prompts = [
    # tokenizer.apply_chat_template( [{'role': 'user', 'content': x}],
    #     tokenize=False, 
    #     add_generation_prompt=True 
    #     ) for x in prompts
    #     ]

    # print(prompts)
    # assert False
    return prompts


if __name__=="__main__":
    task = "asr"
    datasets = ["earning22", "medasr", "voxpopuli", "tedlium"]

    for dataset in datasets:

        # model_id = "PeacefulData/neko-test"
        # llm, tokenizer = get_model(model_id)

        # prompts = [tokenizer.apply_chat_template([{'role': 'user', 'content': "Who are you?"}],
        #     tokenize=False, 
        #     add_generation_prompt=True 
        #     )]

        with open(f"/mnt/home/zhenwan.nlp/ASR-Eval/ASR_results/subset/{dataset}-whisper-large-v2.json", "r") as f:
            data = json.load(f)

        prompts = get_prompt([x["whisper_v2_1best"] + x["whisper_v2_nbest"][1:5] for x in data], task)
        print(prompts[0])
        # assert False
        
        # sampling_params = SamplingParams(temperature=0.0, max_tokens=1024)

        # outputs = llm.generate(prompts, sampling_params)

        for i in tqdm(range(len(prompts))):
            # prompt = outputs[i].prompt
            # generated_text = outputs[i].outputs[0].text
            # print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")

            generated_text = generate_gpt_response("gpt-4o", prompts[i])
            print(f"Generated text: {generated_text}")

            data[i]["whisper_v2_nbest_gpt4o"] = [generated_text]

        with open(f"GER_results/subset/{dataset}_whisper_v2_nbest_gpt4o.json", "w") as f:
            json.dump(data, f, indent=1)

    # outputs = model.generate(**inputs, max_new_tokens=512)
    # print(tokenizer.decode(outputs[0], skip_special_tokens=True))
